Starting Optimization
Creating training net.
Creating Layer data
data -> data
data -> label
Opening leveldb clampdet_train_leveldb
output data size: 128,3,227,227
Loading mean file from../../data/clampdet/clampdet_mean.binaryproto
Top shape: 128 3 227 227 (19787136)
Top shape: 128 1 1 1 (128)
data does not need backward computation.
Creating Layer conv1
conv1 <- data
conv1 -> conv1
Top shape: 128 96 55 55 (37171200)
conv1 does not need backward computation.
Creating Layer relu1
relu1 <- conv1
relu1 -> conv1 (in-place)
Top shape: 128 96 55 55 (37171200)
relu1 does not need backward computation.
Creating Layer pool1
pool1 <- conv1
pool1 -> pool1
Top shape: 128 96 27 27 (8957952)
pool1 does not need backward computation.
Creating Layer norm1
norm1 <- pool1
norm1 -> norm1
Top shape: 128 96 27 27 (8957952)
norm1 does not need backward computation.
Creating Layer conv2
conv2 <- norm1
conv2 -> conv2
Top shape: 128 256 27 27 (23887872)
conv2 needs backward computation.
Creating Layer relu2
relu2 <- conv2
relu2 -> conv2 (in-place)
Top shape: 128 256 27 27 (23887872)
relu2 needs backward computation.
Creating Layer pool2
pool2 <- conv2
pool2 -> pool2
Top shape: 128 256 13 13 (5537792)
pool2 needs backward computation.
Creating Layer norm2
norm2 <- pool2
norm2 -> norm2
Top shape: 128 256 13 13 (5537792)
norm2 needs backward computation.
Creating Layer conv3
conv3 <- norm2
conv3 -> conv3
Top shape: 128 384 13 13 (8306688)
conv3 needs backward computation.
Creating Layer relu3
relu3 <- conv3
relu3 -> conv3 (in-place)
Top shape: 128 384 13 13 (8306688)
relu3 needs backward computation.
Creating Layer conv4
conv4 <- conv3
conv4 -> conv4
Top shape: 128 384 13 13 (8306688)
conv4 needs backward computation.
Creating Layer relu4
relu4 <- conv4
relu4 -> conv4 (in-place)
Top shape: 128 384 13 13 (8306688)
relu4 needs backward computation.
Creating Layer conv5
conv5 <- conv4
conv5 -> conv5
Top shape: 128 256 13 13 (5537792)
conv5 needs backward computation.
Creating Layer relu5
relu5 <- conv5
relu5 -> conv5 (in-place)
Top shape: 128 256 13 13 (5537792)
relu5 needs backward computation.
Creating Layer pool5
pool5 <- conv5
pool5 -> pool5
Top shape: 128 256 6 6 (1179648)
pool5 needs backward computation.
Creating Layer fc6
fc6 <- pool5
fc6 -> fc6
Top shape: 128 4096 1 1 (524288)
fc6 needs backward computation.
Creating Layer relu6
relu6 <- fc6
relu6 -> fc6 (in-place)
Top shape: 128 4096 1 1 (524288)
relu6 needs backward computation.
Creating Layer drop6
drop6 <- fc6
drop6 -> fc6 (in-place)
Top shape: 128 4096 1 1 (524288)
drop6 needs backward computation.
Creating Layer fc7_new
fc7_new <- fc6
fc7_new -> fc7
Top shape: 128 4096 1 1 (524288)
fc7_new needs backward computation.
Creating Layer relu7
relu7 <- fc7
relu7 -> fc7 (in-place)
Top shape: 128 4096 1 1 (524288)
relu7 needs backward computation.
Creating Layer drop7
drop7 <- fc7
drop7 -> fc7 (in-place)
Top shape: 128 4096 1 1 (524288)
drop7 needs backward computation.
Creating Layer fc8_new
fc8_new <- fc7
fc8_new -> fc8_new
Top shape: 128 2 1 1 (256)
fc8_new needs backward computation.
Creating Layer loss
loss <- fc8_new
loss <- label
loss needs backward computation.
Collecting Learning Rate and Weight Decay.
Network initialization done.
Memory required for Data 536869888
Creating testing net.
Creating Layer data
data -> data
data -> label
Opening leveldb clampdet_val_leveldb
output data size: 128,3,227,227
Loading mean file from../../data/clampdet/clampdet_mean.binaryproto
Top shape: 128 3 227 227 (19787136)
Top shape: 128 1 1 1 (128)
data does not need backward computation.
Creating Layer conv1
conv1 <- data
conv1 -> conv1
Top shape: 128 96 55 55 (37171200)
conv1 does not need backward computation.
Creating Layer relu1
relu1 <- conv1
relu1 -> conv1 (in-place)
Top shape: 128 96 55 55 (37171200)
relu1 does not need backward computation.
Creating Layer pool1
pool1 <- conv1
pool1 -> pool1
Top shape: 128 96 27 27 (8957952)
pool1 does not need backward computation.
Creating Layer norm1
norm1 <- pool1
norm1 -> norm1
Top shape: 128 96 27 27 (8957952)
norm1 does not need backward computation.
Creating Layer conv2
conv2 <- norm1
conv2 -> conv2
Top shape: 128 256 27 27 (23887872)
conv2 needs backward computation.
Creating Layer relu2
relu2 <- conv2
relu2 -> conv2 (in-place)
Top shape: 128 256 27 27 (23887872)
relu2 needs backward computation.
Creating Layer pool2
pool2 <- conv2
pool2 -> pool2
Top shape: 128 256 13 13 (5537792)
pool2 needs backward computation.
Creating Layer norm2
norm2 <- pool2
norm2 -> norm2
Top shape: 128 256 13 13 (5537792)
norm2 needs backward computation.
Creating Layer conv3
conv3 <- norm2
conv3 -> conv3
Top shape: 128 384 13 13 (8306688)
conv3 needs backward computation.
Creating Layer relu3
relu3 <- conv3
relu3 -> conv3 (in-place)
Top shape: 128 384 13 13 (8306688)
relu3 needs backward computation.
Creating Layer conv4
conv4 <- conv3
conv4 -> conv4
Top shape: 128 384 13 13 (8306688)
conv4 needs backward computation.
Creating Layer relu4
relu4 <- conv4
relu4 -> conv4 (in-place)
Top shape: 128 384 13 13 (8306688)
relu4 needs backward computation.
Creating Layer conv5
conv5 <- conv4
conv5 -> conv5
Top shape: 128 256 13 13 (5537792)
conv5 needs backward computation.
Creating Layer relu5
relu5 <- conv5
relu5 -> conv5 (in-place)
Top shape: 128 256 13 13 (5537792)
relu5 needs backward computation.
Creating Layer pool5
pool5 <- conv5
pool5 -> pool5
Top shape: 128 256 6 6 (1179648)
pool5 needs backward computation.
Creating Layer fc6
fc6 <- pool5
fc6 -> fc6
Top shape: 128 4096 1 1 (524288)
fc6 needs backward computation.
Creating Layer relu6
relu6 <- fc6
relu6 -> fc6 (in-place)
Top shape: 128 4096 1 1 (524288)
relu6 needs backward computation.
Creating Layer drop6
drop6 <- fc6
drop6 -> fc6 (in-place)
Top shape: 128 4096 1 1 (524288)
drop6 needs backward computation.
Creating Layer fc7
fc7 <- fc6
fc7 -> fc7
Top shape: 128 4096 1 1 (524288)
fc7 needs backward computation.
Creating Layer relu7
relu7 <- fc7
relu7 -> fc7 (in-place)
Top shape: 128 4096 1 1 (524288)
relu7 needs backward computation.
Creating Layer drop7
drop7 <- fc7
drop7 -> fc7 (in-place)
Top shape: 128 4096 1 1 (524288)
drop7 needs backward computation.
Creating Layer fc8_new
fc8_new <- fc7
fc8_new -> fc8_new
Top shape: 128 2 1 1 (256)
fc8_new needs backward computation.
Creating Layer prob
prob <- fc8_new
prob -> prob
Top shape: 128 2 1 1 (256)
prob needs backward computation.
Creating Layer accuracy
accuracy <- prob
accuracy <- label
accuracy -> accuracy
Top shape: 1 2 1 1 (2)
accuracy needs backward computation.
This network produces output accuracy
Collecting Learning Rate and Weight Decay.
Network initialization done.
Memory required for Data 536870920
Solver scaffolding done.
Loading from ../alexnet/caffe_alexnet_model
Copying source layer data
Copying source layer conv1
Copying source layer relu1
Copying source layer norm1
Copying source layer pool1
Copying source layer conv2
Copying source layer relu2
Copying source layer norm2
Copying source layer pool2
Copying source layer conv3
Copying source layer relu3
Copying source layer conv4
Copying source layer relu4
Copying source layer conv5
Copying source layer relu5
Copying source layer pool5
Copying source layer fc6
Copying source layer relu6
Copying source layer drop6
Ignoring source layer fc7
Copying source layer relu7
Copying source layer drop7
Ignoring source layer fc8
Copying source layer loss
