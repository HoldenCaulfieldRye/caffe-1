nohup: ignoring input
I0828 07:10:28.515303  2014 finetune_net.cpp:25] Starting Optimization
I0828 07:10:28.515415  2014 solver.cpp:42] Creating training net.
I0828 07:10:28.516582  2014 net.cpp:76] Creating Layer data
I0828 07:10:28.516599  2014 net.cpp:112] data -> data
I0828 07:10:28.516614  2014 net.cpp:112] data -> label
I0828 07:10:28.517241  2014 data_layer.cpp:145] Opening leveldb clampdet_train_leveldb
I0828 07:10:28.783139  2014 data_layer.cpp:185] output data size: 128,3,227,227
I0828 07:10:28.783160  2014 data_layer.cpp:204] Loading mean file from../../data/clampdet/clampdet_mean.binaryproto
I0828 07:10:33.526979  2014 net.cpp:127] Top shape: 128 3 227 227 (19787136)
I0828 07:10:33.527016  2014 net.cpp:127] Top shape: 128 1 1 1 (128)
I0828 07:10:33.527024  2014 net.cpp:158] data does not need backward computation.
I0828 07:10:33.527040  2014 net.cpp:76] Creating Layer conv1
I0828 07:10:33.527047  2014 net.cpp:86] conv1 <- data
I0828 07:10:33.527067  2014 net.cpp:112] conv1 -> conv1
I0828 07:10:33.611106  2014 net.cpp:127] Top shape: 128 96 55 55 (37171200)
I0828 07:10:33.611127  2014 net.cpp:153] conv1 needs backward computation.
I0828 07:10:33.611137  2014 net.cpp:76] Creating Layer relu1
I0828 07:10:33.611143  2014 net.cpp:86] relu1 <- conv1
I0828 07:10:33.611150  2014 net.cpp:100] relu1 -> conv1 (in-place)
I0828 07:10:33.649922  2014 net.cpp:127] Top shape: 128 96 55 55 (37171200)
I0828 07:10:33.649945  2014 net.cpp:153] relu1 needs backward computation.
I0828 07:10:33.649956  2014 net.cpp:76] Creating Layer pool1
I0828 07:10:33.649962  2014 net.cpp:86] pool1 <- conv1
I0828 07:10:33.649970  2014 net.cpp:112] pool1 -> pool1
I0828 07:10:33.649984  2014 net.cpp:127] Top shape: 128 96 27 27 (8957952)
I0828 07:10:33.649991  2014 net.cpp:153] pool1 needs backward computation.
I0828 07:10:33.649999  2014 net.cpp:76] Creating Layer norm1
I0828 07:10:33.650007  2014 net.cpp:86] norm1 <- pool1
I0828 07:10:33.650012  2014 net.cpp:112] norm1 -> norm1
I0828 07:10:33.650022  2014 net.cpp:127] Top shape: 128 96 27 27 (8957952)
I0828 07:10:33.650027  2014 net.cpp:153] norm1 needs backward computation.
I0828 07:10:33.650034  2014 net.cpp:76] Creating Layer conv2
I0828 07:10:33.650039  2014 net.cpp:86] conv2 <- norm1
I0828 07:10:33.650045  2014 net.cpp:112] conv2 -> conv2
I0828 07:10:33.664933  2014 net.cpp:127] Top shape: 128 256 27 27 (23887872)
I0828 07:10:33.664949  2014 net.cpp:153] conv2 needs backward computation.
I0828 07:10:33.664958  2014 net.cpp:76] Creating Layer relu2
I0828 07:10:33.664963  2014 net.cpp:86] relu2 <- conv2
I0828 07:10:33.664969  2014 net.cpp:100] relu2 -> conv2 (in-place)
I0828 07:10:33.664975  2014 net.cpp:127] Top shape: 128 256 27 27 (23887872)
I0828 07:10:33.664980  2014 net.cpp:153] relu2 needs backward computation.
I0828 07:10:33.664986  2014 net.cpp:76] Creating Layer pool2
I0828 07:10:33.664991  2014 net.cpp:86] pool2 <- conv2
I0828 07:10:33.664998  2014 net.cpp:112] pool2 -> pool2
I0828 07:10:33.665004  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:33.665009  2014 net.cpp:153] pool2 needs backward computation.
I0828 07:10:33.665017  2014 net.cpp:76] Creating Layer norm2
I0828 07:10:33.665022  2014 net.cpp:86] norm2 <- pool2
I0828 07:10:33.665029  2014 net.cpp:112] norm2 -> norm2
I0828 07:10:33.665035  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:33.665040  2014 net.cpp:153] norm2 needs backward computation.
I0828 07:10:33.665047  2014 net.cpp:76] Creating Layer conv3
I0828 07:10:33.665052  2014 net.cpp:86] conv3 <- norm2
I0828 07:10:33.665057  2014 net.cpp:112] conv3 -> conv3
I0828 07:10:33.708142  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:33.708164  2014 net.cpp:153] conv3 needs backward computation.
I0828 07:10:33.708173  2014 net.cpp:76] Creating Layer relu3
I0828 07:10:33.708178  2014 net.cpp:86] relu3 <- conv3
I0828 07:10:33.708185  2014 net.cpp:100] relu3 -> conv3 (in-place)
I0828 07:10:33.708191  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:33.708196  2014 net.cpp:153] relu3 needs backward computation.
I0828 07:10:33.708209  2014 net.cpp:76] Creating Layer conv4
I0828 07:10:33.708214  2014 net.cpp:86] conv4 <- conv3
I0828 07:10:33.708220  2014 net.cpp:112] conv4 -> conv4
I0828 07:10:33.740543  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:33.740561  2014 net.cpp:153] conv4 needs backward computation.
I0828 07:10:33.740569  2014 net.cpp:76] Creating Layer relu4
I0828 07:10:33.740574  2014 net.cpp:86] relu4 <- conv4
I0828 07:10:33.740581  2014 net.cpp:100] relu4 -> conv4 (in-place)
I0828 07:10:33.740586  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:33.740592  2014 net.cpp:153] relu4 needs backward computation.
I0828 07:10:33.740598  2014 net.cpp:76] Creating Layer conv5
I0828 07:10:33.740604  2014 net.cpp:86] conv5 <- conv4
I0828 07:10:33.740609  2014 net.cpp:112] conv5 -> conv5
I0828 07:10:33.762066  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:33.762089  2014 net.cpp:153] conv5 needs backward computation.
I0828 07:10:33.762096  2014 net.cpp:76] Creating Layer relu5
I0828 07:10:33.762102  2014 net.cpp:86] relu5 <- conv5
I0828 07:10:33.762109  2014 net.cpp:100] relu5 -> conv5 (in-place)
I0828 07:10:33.762115  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:33.762120  2014 net.cpp:153] relu5 needs backward computation.
I0828 07:10:33.762127  2014 net.cpp:76] Creating Layer pool5
I0828 07:10:33.762132  2014 net.cpp:86] pool5 <- conv5
I0828 07:10:33.762138  2014 net.cpp:112] pool5 -> pool5
I0828 07:10:33.762146  2014 net.cpp:127] Top shape: 128 256 6 6 (1179648)
I0828 07:10:33.762151  2014 net.cpp:153] pool5 needs backward computation.
I0828 07:10:33.762161  2014 net.cpp:76] Creating Layer fc6
I0828 07:10:33.762167  2014 net.cpp:86] fc6 <- pool5
I0828 07:10:33.762172  2014 net.cpp:112] fc6 -> fc6
I0828 07:10:35.599180  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:35.599213  2014 net.cpp:153] fc6 needs backward computation.
I0828 07:10:35.599225  2014 net.cpp:76] Creating Layer relu6
I0828 07:10:35.599231  2014 net.cpp:86] relu6 <- fc6
I0828 07:10:35.599239  2014 net.cpp:100] relu6 -> fc6 (in-place)
I0828 07:10:35.599246  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:35.599251  2014 net.cpp:153] relu6 needs backward computation.
I0828 07:10:35.599258  2014 net.cpp:76] Creating Layer drop6
I0828 07:10:35.599263  2014 net.cpp:86] drop6 <- fc6
I0828 07:10:35.599269  2014 net.cpp:100] drop6 -> fc6 (in-place)
I0828 07:10:35.599280  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:35.599287  2014 net.cpp:153] drop6 needs backward computation.
I0828 07:10:35.599294  2014 net.cpp:76] Creating Layer fc7
I0828 07:10:35.599298  2014 net.cpp:86] fc7 <- fc6
I0828 07:10:35.599304  2014 net.cpp:112] fc7 -> fc7
I0828 07:10:36.413604  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:36.413632  2014 net.cpp:153] fc7 needs backward computation.
I0828 07:10:36.413643  2014 net.cpp:76] Creating Layer relu7
I0828 07:10:36.413650  2014 net.cpp:86] relu7 <- fc7
I0828 07:10:36.413657  2014 net.cpp:100] relu7 -> fc7 (in-place)
I0828 07:10:36.413664  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:36.413669  2014 net.cpp:153] relu7 needs backward computation.
I0828 07:10:36.413676  2014 net.cpp:76] Creating Layer drop7
I0828 07:10:36.413681  2014 net.cpp:86] drop7 <- fc7
I0828 07:10:36.413686  2014 net.cpp:100] drop7 -> fc7 (in-place)
I0828 07:10:36.413692  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:36.413697  2014 net.cpp:153] drop7 needs backward computation.
I0828 07:10:36.413707  2014 net.cpp:76] Creating Layer fc8_new
I0828 07:10:36.413712  2014 net.cpp:86] fc8_new <- fc7
I0828 07:10:36.413717  2014 net.cpp:112] fc8_new -> fc8_new
I0828 07:10:36.414139  2014 net.cpp:127] Top shape: 128 2 1 1 (256)
I0828 07:10:36.414150  2014 net.cpp:153] fc8_new needs backward computation.
I0828 07:10:36.414157  2014 net.cpp:76] Creating Layer loss
I0828 07:10:36.414163  2014 net.cpp:86] loss <- fc8_new
I0828 07:10:36.414170  2014 net.cpp:86] loss <- label
I0828 07:10:36.414180  2014 net.cpp:153] loss needs backward computation.
I0828 07:10:36.414212  2014 net.cpp:182] Collecting Learning Rate and Weight Decay.
I0828 07:10:36.414226  2014 net.cpp:175] Network initialization done.
I0828 07:10:36.414230  2014 net.cpp:176] Memory required for Data 536869888
I0828 07:10:36.414278  2014 solver.cpp:45] Creating testing net.
I0828 07:10:36.414976  2014 net.cpp:76] Creating Layer data
I0828 07:10:36.414990  2014 net.cpp:112] data -> data
I0828 07:10:36.414999  2014 net.cpp:112] data -> label
I0828 07:10:36.415007  2014 data_layer.cpp:145] Opening leveldb clampdet_val_leveldb
I0828 07:10:36.735296  2014 data_layer.cpp:185] output data size: 128,3,227,227
I0828 07:10:36.735324  2014 data_layer.cpp:204] Loading mean file from../../data/clampdet/clampdet_mean.binaryproto
I0828 07:10:36.779800  2014 net.cpp:127] Top shape: 128 3 227 227 (19787136)
I0828 07:10:36.779873  2014 net.cpp:127] Top shape: 128 1 1 1 (128)
I0828 07:10:36.779906  2014 net.cpp:158] data does not need backward computation.
I0828 07:10:36.779937  2014 net.cpp:76] Creating Layer conv1
I0828 07:10:36.779963  2014 net.cpp:86] conv1 <- data
I0828 07:10:36.779991  2014 net.cpp:112] conv1 -> conv1
I0828 07:10:36.781690  2014 net.cpp:127] Top shape: 128 96 55 55 (37171200)
I0828 07:10:36.781704  2014 net.cpp:153] conv1 needs backward computation.
I0828 07:10:36.781713  2014 net.cpp:76] Creating Layer relu1
I0828 07:10:36.781719  2014 net.cpp:86] relu1 <- conv1
I0828 07:10:36.781726  2014 net.cpp:100] relu1 -> conv1 (in-place)
I0828 07:10:36.781733  2014 net.cpp:127] Top shape: 128 96 55 55 (37171200)
I0828 07:10:36.781738  2014 net.cpp:153] relu1 needs backward computation.
I0828 07:10:36.781744  2014 net.cpp:76] Creating Layer pool1
I0828 07:10:36.781749  2014 net.cpp:86] pool1 <- conv1
I0828 07:10:36.781755  2014 net.cpp:112] pool1 -> pool1
I0828 07:10:36.781762  2014 net.cpp:127] Top shape: 128 96 27 27 (8957952)
I0828 07:10:36.781767  2014 net.cpp:153] pool1 needs backward computation.
I0828 07:10:36.781775  2014 net.cpp:76] Creating Layer norm1
I0828 07:10:36.781781  2014 net.cpp:86] norm1 <- pool1
I0828 07:10:36.781786  2014 net.cpp:112] norm1 -> norm1
I0828 07:10:36.781795  2014 net.cpp:127] Top shape: 128 96 27 27 (8957952)
I0828 07:10:36.781800  2014 net.cpp:153] norm1 needs backward computation.
I0828 07:10:36.781807  2014 net.cpp:76] Creating Layer conv2
I0828 07:10:36.781813  2014 net.cpp:86] conv2 <- norm1
I0828 07:10:36.781818  2014 net.cpp:112] conv2 -> conv2
I0828 07:10:36.796470  2014 net.cpp:127] Top shape: 128 256 27 27 (23887872)
I0828 07:10:36.796494  2014 net.cpp:153] conv2 needs backward computation.
I0828 07:10:36.796504  2014 net.cpp:76] Creating Layer relu2
I0828 07:10:36.796509  2014 net.cpp:86] relu2 <- conv2
I0828 07:10:36.796516  2014 net.cpp:100] relu2 -> conv2 (in-place)
I0828 07:10:36.796524  2014 net.cpp:127] Top shape: 128 256 27 27 (23887872)
I0828 07:10:36.796529  2014 net.cpp:153] relu2 needs backward computation.
I0828 07:10:36.796535  2014 net.cpp:76] Creating Layer pool2
I0828 07:10:36.796540  2014 net.cpp:86] pool2 <- conv2
I0828 07:10:36.796545  2014 net.cpp:112] pool2 -> pool2
I0828 07:10:36.796553  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:36.796558  2014 net.cpp:153] pool2 needs backward computation.
I0828 07:10:36.796567  2014 net.cpp:76] Creating Layer norm2
I0828 07:10:36.796572  2014 net.cpp:86] norm2 <- pool2
I0828 07:10:36.796577  2014 net.cpp:112] norm2 -> norm2
I0828 07:10:36.796584  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:36.796589  2014 net.cpp:153] norm2 needs backward computation.
I0828 07:10:36.796597  2014 net.cpp:76] Creating Layer conv3
I0828 07:10:36.796602  2014 net.cpp:86] conv3 <- norm2
I0828 07:10:36.796607  2014 net.cpp:112] conv3 -> conv3
I0828 07:10:36.839958  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:36.839984  2014 net.cpp:153] conv3 needs backward computation.
I0828 07:10:36.839994  2014 net.cpp:76] Creating Layer relu3
I0828 07:10:36.840001  2014 net.cpp:86] relu3 <- conv3
I0828 07:10:36.840008  2014 net.cpp:100] relu3 -> conv3 (in-place)
I0828 07:10:36.840014  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:36.840019  2014 net.cpp:153] relu3 needs backward computation.
I0828 07:10:36.840026  2014 net.cpp:76] Creating Layer conv4
I0828 07:10:36.840031  2014 net.cpp:86] conv4 <- conv3
I0828 07:10:36.840037  2014 net.cpp:112] conv4 -> conv4
I0828 07:10:36.872387  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:36.872406  2014 net.cpp:153] conv4 needs backward computation.
I0828 07:10:36.872416  2014 net.cpp:76] Creating Layer relu4
I0828 07:10:36.872421  2014 net.cpp:86] relu4 <- conv4
I0828 07:10:36.872427  2014 net.cpp:100] relu4 -> conv4 (in-place)
I0828 07:10:36.872433  2014 net.cpp:127] Top shape: 128 384 13 13 (8306688)
I0828 07:10:36.872438  2014 net.cpp:153] relu4 needs backward computation.
I0828 07:10:36.872447  2014 net.cpp:76] Creating Layer conv5
I0828 07:10:36.872452  2014 net.cpp:86] conv5 <- conv4
I0828 07:10:36.872457  2014 net.cpp:112] conv5 -> conv5
I0828 07:10:36.893899  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:36.893918  2014 net.cpp:153] conv5 needs backward computation.
I0828 07:10:36.893925  2014 net.cpp:76] Creating Layer relu5
I0828 07:10:36.893931  2014 net.cpp:86] relu5 <- conv5
I0828 07:10:36.893937  2014 net.cpp:100] relu5 -> conv5 (in-place)
I0828 07:10:36.893944  2014 net.cpp:127] Top shape: 128 256 13 13 (5537792)
I0828 07:10:36.893949  2014 net.cpp:153] relu5 needs backward computation.
I0828 07:10:36.893955  2014 net.cpp:76] Creating Layer pool5
I0828 07:10:36.893960  2014 net.cpp:86] pool5 <- conv5
I0828 07:10:36.893966  2014 net.cpp:112] pool5 -> pool5
I0828 07:10:36.893975  2014 net.cpp:127] Top shape: 128 256 6 6 (1179648)
I0828 07:10:36.893980  2014 net.cpp:153] pool5 needs backward computation.
I0828 07:10:36.893990  2014 net.cpp:76] Creating Layer fc6
I0828 07:10:36.893996  2014 net.cpp:86] fc6 <- pool5
I0828 07:10:36.894001  2014 net.cpp:112] fc6 -> fc6
I0828 07:10:38.736009  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:38.736042  2014 net.cpp:153] fc6 needs backward computation.
I0828 07:10:38.736055  2014 net.cpp:76] Creating Layer relu6
I0828 07:10:38.736063  2014 net.cpp:86] relu6 <- fc6
I0828 07:10:38.736071  2014 net.cpp:100] relu6 -> fc6 (in-place)
I0828 07:10:38.736078  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:38.736084  2014 net.cpp:153] relu6 needs backward computation.
I0828 07:10:38.736091  2014 net.cpp:76] Creating Layer drop6
I0828 07:10:38.736096  2014 net.cpp:86] drop6 <- fc6
I0828 07:10:38.736101  2014 net.cpp:100] drop6 -> fc6 (in-place)
I0828 07:10:38.736109  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:38.736114  2014 net.cpp:153] drop6 needs backward computation.
I0828 07:10:38.736121  2014 net.cpp:76] Creating Layer fc7
I0828 07:10:38.736126  2014 net.cpp:86] fc7 <- fc6
I0828 07:10:38.736132  2014 net.cpp:112] fc7 -> fc7
I0828 07:10:39.555565  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:39.555596  2014 net.cpp:153] fc7 needs backward computation.
I0828 07:10:39.555608  2014 net.cpp:76] Creating Layer relu7
I0828 07:10:39.555614  2014 net.cpp:86] relu7 <- fc7
I0828 07:10:39.555623  2014 net.cpp:100] relu7 -> fc7 (in-place)
I0828 07:10:39.555629  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:39.555634  2014 net.cpp:153] relu7 needs backward computation.
I0828 07:10:39.555641  2014 net.cpp:76] Creating Layer drop7
I0828 07:10:39.555646  2014 net.cpp:86] drop7 <- fc7
I0828 07:10:39.555651  2014 net.cpp:100] drop7 -> fc7 (in-place)
I0828 07:10:39.555658  2014 net.cpp:127] Top shape: 128 4096 1 1 (524288)
I0828 07:10:39.555663  2014 net.cpp:153] drop7 needs backward computation.
I0828 07:10:39.555671  2014 net.cpp:76] Creating Layer fc8_new
I0828 07:10:39.555676  2014 net.cpp:86] fc8_new <- fc7
I0828 07:10:39.555682  2014 net.cpp:112] fc8_new -> fc8_new
I0828 07:10:39.556098  2014 net.cpp:127] Top shape: 128 2 1 1 (256)
I0828 07:10:39.556109  2014 net.cpp:153] fc8_new needs backward computation.
I0828 07:10:39.556116  2014 net.cpp:76] Creating Layer prob
I0828 07:10:39.556121  2014 net.cpp:86] prob <- fc8_new
I0828 07:10:39.556128  2014 net.cpp:112] prob -> prob
I0828 07:10:39.556136  2014 net.cpp:127] Top shape: 128 2 1 1 (256)
I0828 07:10:39.556143  2014 net.cpp:153] prob needs backward computation.
I0828 07:10:39.556149  2014 net.cpp:76] Creating Layer accuracy
I0828 07:10:39.556154  2014 net.cpp:86] accuracy <- prob
I0828 07:10:39.556159  2014 net.cpp:86] accuracy <- label
I0828 07:10:39.556167  2014 net.cpp:112] accuracy -> accuracy
I0828 07:10:39.583114  2014 net.cpp:127] Top shape: 1 2 1 1 (2)
I0828 07:10:39.583137  2014 net.cpp:153] accuracy needs backward computation.
I0828 07:10:39.583155  2014 net.cpp:164] This network produces output accuracy
I0828 07:10:39.583180  2014 net.cpp:182] Collecting Learning Rate and Weight Decay.
I0828 07:10:39.583194  2014 net.cpp:175] Network initialization done.
I0828 07:10:39.583199  2014 net.cpp:176] Memory required for Data 536870920
I0828 07:10:39.583246  2014 solver.cpp:50] Solver scaffolding done.
I0828 07:10:39.583256  2014 finetune_net.cpp:27] Loading from ../alexnet/caffe_alexnet_model
I0828 07:11:17.912066  2014 net.cpp:366] Copying source layer data
I0828 07:11:18.037734  2014 net.cpp:366] Copying source layer conv1
I0828 07:11:19.337776  2014 net.cpp:366] Copying source layer relu1
I0828 07:11:19.337803  2014 net.cpp:366] Copying source layer norm1
I0828 07:11:19.337810  2014 net.cpp:366] Copying source layer pool1
I0828 07:11:19.337815  2014 net.cpp:366] Copying source layer conv2
I0828 07:11:19.338621  2014 net.cpp:366] Copying source layer relu2
I0828 07:11:19.338639  2014 net.cpp:366] Copying source layer norm2
I0828 07:11:19.338644  2014 net.cpp:366] Copying source layer pool2
I0828 07:11:19.338649  2014 net.cpp:366] Copying source layer conv3
I0828 07:11:19.340839  2014 net.cpp:366] Copying source layer relu3
I0828 07:11:19.340865  2014 net.cpp:366] Copying source layer conv4
I0828 07:11:19.342514  2014 net.cpp:366] Copying source layer relu4
I0828 07:11:19.342532  2014 net.cpp:366] Copying source layer conv5
I0828 07:11:19.343685  2014 net.cpp:366] Copying source layer relu5
I0828 07:11:19.343701  2014 net.cpp:366] Copying source layer pool5
I0828 07:11:19.343706  2014 net.cpp:366] Copying source layer fc6
I0828 07:11:19.498915  2014 net.cpp:366] Copying source layer relu6
I0828 07:11:19.498953  2014 net.cpp:366] Copying source layer drop6
I0828 07:11:19.498960  2014 net.cpp:366] Copying source layer fc7
I0828 07:11:19.564498  2014 net.cpp:366] Copying source layer relu7
I0828 07:11:19.564532  2014 net.cpp:366] Copying source layer drop7
I0828 07:11:19.564537  2014 net.cpp:363] Ignoring source layer fc8
I0828 07:11:19.564542  2014 net.cpp:366] Copying source layer loss
I0828 07:11:19.584664  2014 solver.cpp:62] Solving ClampFineNet
I0828 07:11:19.620666  2014 solver.cpp:136] Iteration 0, Testing net
I0828 07:11:50.142917  2014 solver.cpp:172] Test score #0: 0.484006
I0828 07:11:50.152237  2014 solver.cpp:172] Test score #1: 0.72543
I0828 07:11:50.778497  2014 solver.cpp:269] Iteration 1, lr = 0.0001
F0828 07:11:50.907649  2014 syncedmem.cpp:47] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
    @     0x7ff2175ecf9d  google::LogMessage::Fail()
    @     0x7ff2175ef0af  google::LogMessage::SendToLog()
    @     0x7ff2175ecb8c  google::LogMessage::Flush()
    @     0x7ff2175ef94d  google::LogMessageFatal::~LogMessageFatal()
    @           0x4bbd57  caffe::SyncedMemory::mutable_gpu_data()
    @           0x45fcb1  caffe::Blob<>::mutable_gpu_data()
    @           0x4464a0  caffe::SGDSolver<>::ComputeUpdateValue()
    @           0x448791  caffe::Solver<>::Solve()
    @           0x412976  main
    @     0x7ff215184ea5  (unknown)
    @           0x414879  (unknown)
Aborted
Done.
