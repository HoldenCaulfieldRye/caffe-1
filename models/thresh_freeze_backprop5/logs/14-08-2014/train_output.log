nohup: ignoring input
I0813 18:38:57.033383 16416 finetune_net.cpp:25] Starting Optimization
I0813 18:38:57.033524 16416 solver.cpp:41] Creating training net.
I0813 18:38:57.034484 16416 net.cpp:75] Creating Layer data
I0813 18:38:57.034497 16416 net.cpp:111] data -> data
I0813 18:38:57.034514 16416 net.cpp:111] data -> label
I0813 18:38:57.034548 16416 data_layer.cpp:145] Opening leveldb thresh_fine_train_leveldb
I0813 18:39:05.859333 16416 data_layer.cpp:185] output data size: 256,3,227,227
I0813 18:39:05.859416 16416 data_layer.cpp:204] Loading mean file from../../data/thresh/thresh_fine_mean.binaryproto
I0813 18:39:06.166877 16416 net.cpp:126] Top shape: 256 3 227 227 (39574272)
I0813 18:39:06.166997 16416 net.cpp:126] Top shape: 256 1 1 1 (256)
I0813 18:39:06.167039 16416 net.cpp:157] data does not need backward computation.
I0813 18:39:06.167089 16416 net.cpp:75] Creating Layer conv1
I0813 18:39:06.167129 16416 net.cpp:85] conv1 <- data
I0813 18:39:06.167181 16416 net.cpp:111] conv1 -> conv1
I0813 18:39:06.169381 16416 net.cpp:126] Top shape: 256 96 55 55 (74342400)
I0813 18:39:06.169473 16416 net.cpp:157] conv1 does not need backward computation.
I0813 18:39:06.169517 16416 net.cpp:75] Creating Layer relu1
I0813 18:39:06.169560 16416 net.cpp:85] relu1 <- conv1
I0813 18:39:06.169596 16416 net.cpp:99] relu1 -> conv1 (in-place)
I0813 18:39:06.169641 16416 net.cpp:126] Top shape: 256 96 55 55 (74342400)
I0813 18:39:06.169679 16416 net.cpp:157] relu1 does not need backward computation.
I0813 18:39:06.169723 16416 net.cpp:75] Creating Layer pool1
I0813 18:39:06.169755 16416 net.cpp:85] pool1 <- conv1
I0813 18:39:06.169800 16416 net.cpp:111] pool1 -> pool1
I0813 18:39:06.169847 16416 net.cpp:126] Top shape: 256 96 27 27 (17915904)
I0813 18:39:06.169891 16416 net.cpp:157] pool1 does not need backward computation.
I0813 18:39:06.169939 16416 net.cpp:75] Creating Layer norm1
I0813 18:39:06.169976 16416 net.cpp:85] norm1 <- pool1
I0813 18:39:06.170013 16416 net.cpp:111] norm1 -> norm1
I0813 18:39:06.170055 16416 net.cpp:126] Top shape: 256 96 27 27 (17915904)
I0813 18:39:06.170090 16416 net.cpp:157] norm1 does not need backward computation.
I0813 18:39:06.170132 16416 net.cpp:75] Creating Layer conv2
I0813 18:39:06.170167 16416 net.cpp:85] conv2 <- norm1
I0813 18:39:06.170202 16416 net.cpp:111] conv2 -> conv2
I0813 18:39:06.188688 16416 net.cpp:126] Top shape: 256 256 27 27 (47775744)
I0813 18:39:06.188771 16416 net.cpp:157] conv2 does not need backward computation.
I0813 18:39:06.188804 16416 net.cpp:75] Creating Layer relu2
I0813 18:39:06.188835 16416 net.cpp:85] relu2 <- conv2
I0813 18:39:06.188865 16416 net.cpp:99] relu2 -> conv2 (in-place)
I0813 18:39:06.188896 16416 net.cpp:126] Top shape: 256 256 27 27 (47775744)
I0813 18:39:06.188938 16416 net.cpp:157] relu2 does not need backward computation.
I0813 18:39:06.188978 16416 net.cpp:75] Creating Layer pool2
I0813 18:39:06.189007 16416 net.cpp:85] pool2 <- conv2
I0813 18:39:06.189033 16416 net.cpp:111] pool2 -> pool2
I0813 18:39:06.189064 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:06.189095 16416 net.cpp:157] pool2 does not need backward computation.
I0813 18:39:06.189131 16416 net.cpp:75] Creating Layer norm2
I0813 18:39:06.189160 16416 net.cpp:85] norm2 <- pool2
I0813 18:39:06.189187 16416 net.cpp:111] norm2 -> norm2
I0813 18:39:06.189215 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:06.189245 16416 net.cpp:157] norm2 does not need backward computation.
I0813 18:39:06.189280 16416 net.cpp:75] Creating Layer conv3
I0813 18:39:06.189311 16416 net.cpp:85] conv3 <- norm2
I0813 18:39:06.189344 16416 net.cpp:111] conv3 -> conv3
I0813 18:39:06.241827 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:06.241926 16416 net.cpp:157] conv3 does not need backward computation.
I0813 18:39:06.241968 16416 net.cpp:75] Creating Layer relu3
I0813 18:39:06.241997 16416 net.cpp:85] relu3 <- conv3
I0813 18:39:06.242027 16416 net.cpp:99] relu3 -> conv3 (in-place)
I0813 18:39:06.242054 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:06.242095 16416 net.cpp:157] relu3 does not need backward computation.
I0813 18:39:06.242126 16416 net.cpp:75] Creating Layer conv4
I0813 18:39:06.242161 16416 net.cpp:85] conv4 <- conv3
I0813 18:39:06.242197 16416 net.cpp:111] conv4 -> conv4
I0813 18:39:06.280678 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:06.280764 16416 net.cpp:157] conv4 does not need backward computation.
I0813 18:39:06.280797 16416 net.cpp:75] Creating Layer relu4
I0813 18:39:06.280823 16416 net.cpp:85] relu4 <- conv4
I0813 18:39:06.280853 16416 net.cpp:99] relu4 -> conv4 (in-place)
I0813 18:39:06.280881 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:06.280913 16416 net.cpp:157] relu4 does not need backward computation.
I0813 18:39:06.280946 16416 net.cpp:75] Creating Layer conv5
I0813 18:39:06.280975 16416 net.cpp:85] conv5 <- conv4
I0813 18:39:06.281002 16416 net.cpp:111] conv5 -> conv5
I0813 18:39:06.306171 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:06.306252 16416 net.cpp:157] conv5 does not need backward computation.
I0813 18:39:06.306283 16416 net.cpp:75] Creating Layer relu5
I0813 18:39:06.306310 16416 net.cpp:85] relu5 <- conv5
I0813 18:39:06.306339 16416 net.cpp:99] relu5 -> conv5 (in-place)
I0813 18:39:06.306366 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:06.306397 16416 net.cpp:157] relu5 does not need backward computation.
I0813 18:39:06.306445 16416 net.cpp:75] Creating Layer pool5
I0813 18:39:06.306483 16416 net.cpp:85] pool5 <- conv5
I0813 18:39:06.306515 16416 net.cpp:111] pool5 -> pool5
I0813 18:39:06.306550 16416 net.cpp:126] Top shape: 256 256 6 6 (2359296)
I0813 18:39:06.306581 16416 net.cpp:157] pool5 does not need backward computation.
I0813 18:39:06.306617 16416 net.cpp:75] Creating Layer fc6
I0813 18:39:06.306648 16416 net.cpp:85] fc6 <- pool5
I0813 18:39:06.306674 16416 net.cpp:111] fc6 -> fc6
I0813 18:39:08.511374 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:08.511472 16416 net.cpp:152] fc6 needs backward computation.
I0813 18:39:08.511574 16416 net.cpp:75] Creating Layer relu6
I0813 18:39:08.511605 16416 net.cpp:85] relu6 <- fc6
I0813 18:39:08.511644 16416 net.cpp:99] relu6 -> fc6 (in-place)
I0813 18:39:08.511672 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:08.511706 16416 net.cpp:152] relu6 needs backward computation.
I0813 18:39:08.511742 16416 net.cpp:75] Creating Layer drop6
I0813 18:39:08.511776 16416 net.cpp:85] drop6 <- fc6
I0813 18:39:08.511803 16416 net.cpp:99] drop6 -> fc6 (in-place)
I0813 18:39:08.511842 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:08.511873 16416 net.cpp:152] drop6 needs backward computation.
I0813 18:39:08.511904 16416 net.cpp:75] Creating Layer fc7
I0813 18:39:08.511932 16416 net.cpp:85] fc7 <- fc6
I0813 18:39:08.511961 16416 net.cpp:111] fc7 -> fc7
I0813 18:39:09.492758 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:09.492848 16416 net.cpp:152] fc7 needs backward computation.
I0813 18:39:09.492882 16416 net.cpp:75] Creating Layer relu7
I0813 18:39:09.492909 16416 net.cpp:85] relu7 <- fc7
I0813 18:39:09.492975 16416 net.cpp:99] relu7 -> fc7 (in-place)
I0813 18:39:09.493010 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:09.493042 16416 net.cpp:152] relu7 needs backward computation.
I0813 18:39:09.493077 16416 net.cpp:75] Creating Layer drop7
I0813 18:39:09.493110 16416 net.cpp:85] drop7 <- fc7
I0813 18:39:09.493139 16416 net.cpp:99] drop7 -> fc7 (in-place)
I0813 18:39:09.493170 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:09.493201 16416 net.cpp:152] drop7 needs backward computation.
I0813 18:39:09.493232 16416 net.cpp:75] Creating Layer fc8_new
I0813 18:39:09.493260 16416 net.cpp:85] fc8_new <- fc7
I0813 18:39:09.493290 16416 net.cpp:111] fc8_new -> fc8_new
I0813 18:39:09.493789 16416 net.cpp:126] Top shape: 256 2 1 1 (512)
I0813 18:39:09.493821 16416 net.cpp:152] fc8_new needs backward computation.
I0813 18:39:09.493854 16416 net.cpp:75] Creating Layer threshold
I0813 18:39:09.493880 16416 net.cpp:85] threshold <- fc8_new
I0813 18:39:09.493924 16416 net.cpp:85] threshold <- label
I0813 18:39:09.493957 16416 net.cpp:111] threshold -> fc8_new_thresh
I0813 18:39:09.493984 16416 net.cpp:99] threshold -> label (in-place)
I0813 18:39:09.494017 16416 net.cpp:126] Top shape: 256 2 1 1 (512)
I0813 18:39:09.494046 16416 net.cpp:126] Top shape: 256 1 1 1 (256)
I0813 18:39:09.494076 16416 net.cpp:152] threshold needs backward computation.
I0813 18:39:09.494103 16416 net.cpp:75] Creating Layer loss
I0813 18:39:09.494129 16416 net.cpp:85] loss <- fc8_new_thresh
I0813 18:39:09.494156 16416 net.cpp:85] loss <- label
I0813 18:39:09.494187 16416 net.cpp:152] loss needs backward computation.
I0813 18:39:09.494251 16416 net.cpp:181] Collecting Learning Rate and Weight Decay.
I0813 18:39:09.494292 16416 net.cpp:174] Network initialization done.
I0813 18:39:09.494319 16416 net.cpp:175] Memory required for Data 1073739776
I0813 18:39:09.494406 16416 solver.cpp:44] Creating testing net.
I0813 18:39:09.495478 16416 net.cpp:75] Creating Layer data
I0813 18:39:09.495520 16416 net.cpp:111] data -> data
I0813 18:39:09.495551 16416 net.cpp:111] data -> label
I0813 18:39:09.495581 16416 data_layer.cpp:145] Opening leveldb thresh_fine_val_leveldb
I0813 18:39:12.421041 16416 data_layer.cpp:185] output data size: 256,3,227,227
I0813 18:39:12.421262 16416 data_layer.cpp:204] Loading mean file from../../data/thresh/thresh_fine_mean.binaryproto
I0813 18:39:12.443636 16416 net.cpp:126] Top shape: 256 3 227 227 (39574272)
I0813 18:39:12.443861 16416 net.cpp:126] Top shape: 256 1 1 1 (256)
I0813 18:39:12.443976 16416 net.cpp:157] data does not need backward computation.
I0813 18:39:12.444104 16416 net.cpp:75] Creating Layer conv1
I0813 18:39:12.444221 16416 net.cpp:85] conv1 <- data
I0813 18:39:12.444339 16416 net.cpp:111] conv1 -> conv1
I0813 18:39:12.446385 16416 net.cpp:126] Top shape: 256 96 55 55 (74342400)
I0813 18:39:12.446560 16416 net.cpp:152] conv1 needs backward computation.
I0813 18:39:12.446683 16416 net.cpp:75] Creating Layer relu1
I0813 18:39:12.446796 16416 net.cpp:85] relu1 <- conv1
I0813 18:39:12.446909 16416 net.cpp:99] relu1 -> conv1 (in-place)
I0813 18:39:12.447024 16416 net.cpp:126] Top shape: 256 96 55 55 (74342400)
I0813 18:39:12.447141 16416 net.cpp:152] relu1 needs backward computation.
I0813 18:39:12.447258 16416 net.cpp:75] Creating Layer pool1
I0813 18:39:12.447372 16416 net.cpp:85] pool1 <- conv1
I0813 18:39:12.447489 16416 net.cpp:111] pool1 -> pool1
I0813 18:39:12.447713 16416 net.cpp:126] Top shape: 256 96 27 27 (17915904)
I0813 18:39:12.447828 16416 net.cpp:152] pool1 needs backward computation.
I0813 18:39:12.447945 16416 net.cpp:75] Creating Layer norm1
I0813 18:39:12.448057 16416 net.cpp:85] norm1 <- pool1
I0813 18:39:12.448170 16416 net.cpp:111] norm1 -> norm1
I0813 18:39:12.448287 16416 net.cpp:126] Top shape: 256 96 27 27 (17915904)
I0813 18:39:12.448405 16416 net.cpp:152] norm1 needs backward computation.
I0813 18:39:12.448523 16416 net.cpp:75] Creating Layer conv2
I0813 18:39:12.448636 16416 net.cpp:85] conv2 <- norm1
I0813 18:39:12.448750 16416 net.cpp:111] conv2 -> conv2
I0813 18:39:12.465853 16416 net.cpp:126] Top shape: 256 256 27 27 (47775744)
I0813 18:39:12.466074 16416 net.cpp:152] conv2 needs backward computation.
I0813 18:39:12.466193 16416 net.cpp:75] Creating Layer relu2
I0813 18:39:12.466305 16416 net.cpp:85] relu2 <- conv2
I0813 18:39:12.466419 16416 net.cpp:99] relu2 -> conv2 (in-place)
I0813 18:39:12.466549 16416 net.cpp:126] Top shape: 256 256 27 27 (47775744)
I0813 18:39:12.466667 16416 net.cpp:152] relu2 needs backward computation.
I0813 18:39:12.466780 16416 net.cpp:75] Creating Layer pool2
I0813 18:39:12.466892 16416 net.cpp:85] pool2 <- conv2
I0813 18:39:12.467005 16416 net.cpp:111] pool2 -> pool2
I0813 18:39:12.467114 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:12.467203 16416 net.cpp:152] pool2 needs backward computation.
I0813 18:39:12.467325 16416 net.cpp:75] Creating Layer norm2
I0813 18:39:12.467437 16416 net.cpp:85] norm2 <- pool2
I0813 18:39:12.467548 16416 net.cpp:111] norm2 -> norm2
I0813 18:39:12.467681 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:12.467798 16416 net.cpp:152] norm2 needs backward computation.
I0813 18:39:12.467913 16416 net.cpp:75] Creating Layer conv3
I0813 18:39:12.468026 16416 net.cpp:85] conv3 <- norm2
I0813 18:39:12.468139 16416 net.cpp:111] conv3 -> conv3
I0813 18:39:12.516778 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:12.516998 16416 net.cpp:152] conv3 needs backward computation.
I0813 18:39:12.517117 16416 net.cpp:75] Creating Layer relu3
I0813 18:39:12.517230 16416 net.cpp:85] relu3 <- conv3
I0813 18:39:12.517343 16416 net.cpp:99] relu3 -> conv3 (in-place)
I0813 18:39:12.517457 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:12.517570 16416 net.cpp:152] relu3 needs backward computation.
I0813 18:39:12.517683 16416 net.cpp:75] Creating Layer conv4
I0813 18:39:12.517796 16416 net.cpp:85] conv4 <- conv3
I0813 18:39:12.517909 16416 net.cpp:111] conv4 -> conv4
I0813 18:39:12.554589 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:12.554812 16416 net.cpp:152] conv4 needs backward computation.
I0813 18:39:12.554934 16416 net.cpp:75] Creating Layer relu4
I0813 18:39:12.555047 16416 net.cpp:85] relu4 <- conv4
I0813 18:39:12.555162 16416 net.cpp:99] relu4 -> conv4 (in-place)
I0813 18:39:12.555279 16416 net.cpp:126] Top shape: 256 384 13 13 (16613376)
I0813 18:39:12.555390 16416 net.cpp:152] relu4 needs backward computation.
I0813 18:39:12.555507 16416 net.cpp:75] Creating Layer conv5
I0813 18:39:12.555626 16416 net.cpp:85] conv5 <- conv4
I0813 18:39:12.555739 16416 net.cpp:111] conv5 -> conv5
I0813 18:39:12.580247 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:12.580476 16416 net.cpp:152] conv5 needs backward computation.
I0813 18:39:12.580595 16416 net.cpp:75] Creating Layer relu5
I0813 18:39:12.580709 16416 net.cpp:85] relu5 <- conv5
I0813 18:39:12.580824 16416 net.cpp:99] relu5 -> conv5 (in-place)
I0813 18:39:12.580939 16416 net.cpp:126] Top shape: 256 256 13 13 (11075584)
I0813 18:39:12.581055 16416 net.cpp:152] relu5 needs backward computation.
I0813 18:39:12.581171 16416 net.cpp:75] Creating Layer pool5
I0813 18:39:12.581284 16416 net.cpp:85] pool5 <- conv5
I0813 18:39:12.581395 16416 net.cpp:111] pool5 -> pool5
I0813 18:39:12.581511 16416 net.cpp:126] Top shape: 256 256 6 6 (2359296)
I0813 18:39:12.581629 16416 net.cpp:152] pool5 needs backward computation.
I0813 18:39:12.581748 16416 net.cpp:75] Creating Layer fc6
I0813 18:39:12.581861 16416 net.cpp:85] fc6 <- pool5
I0813 18:39:12.581974 16416 net.cpp:111] fc6 -> fc6
I0813 18:39:14.782845 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:14.782949 16416 net.cpp:152] fc6 needs backward computation.
I0813 18:39:14.782989 16416 net.cpp:75] Creating Layer relu6
I0813 18:39:14.783030 16416 net.cpp:85] relu6 <- fc6
I0813 18:39:14.783087 16416 net.cpp:99] relu6 -> fc6 (in-place)
I0813 18:39:14.783139 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:14.783187 16416 net.cpp:152] relu6 needs backward computation.
I0813 18:39:14.783238 16416 net.cpp:75] Creating Layer drop6
I0813 18:39:14.783285 16416 net.cpp:85] drop6 <- fc6
I0813 18:39:14.783334 16416 net.cpp:99] drop6 -> fc6 (in-place)
I0813 18:39:14.783383 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:14.783432 16416 net.cpp:152] drop6 needs backward computation.
I0813 18:39:14.783483 16416 net.cpp:75] Creating Layer fc7
I0813 18:39:14.783529 16416 net.cpp:85] fc7 <- fc6
I0813 18:39:14.783570 16416 net.cpp:111] fc7 -> fc7
I0813 18:39:15.769389 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:15.769423 16416 net.cpp:152] fc7 needs backward computation.
I0813 18:39:15.769433 16416 net.cpp:75] Creating Layer relu7
I0813 18:39:15.769438 16416 net.cpp:85] relu7 <- fc7
I0813 18:39:15.769446 16416 net.cpp:99] relu7 -> fc7 (in-place)
I0813 18:39:15.769451 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:15.769456 16416 net.cpp:152] relu7 needs backward computation.
I0813 18:39:15.769461 16416 net.cpp:75] Creating Layer drop7
I0813 18:39:15.769466 16416 net.cpp:85] drop7 <- fc7
I0813 18:39:15.769470 16416 net.cpp:99] drop7 -> fc7 (in-place)
I0813 18:39:15.769475 16416 net.cpp:126] Top shape: 256 4096 1 1 (1048576)
I0813 18:39:15.769480 16416 net.cpp:152] drop7 needs backward computation.
I0813 18:39:15.769486 16416 net.cpp:75] Creating Layer fc8_new_clamp
I0813 18:39:15.769491 16416 net.cpp:85] fc8_new_clamp <- fc7
I0813 18:39:15.769495 16416 net.cpp:111] fc8_new_clamp -> fc8_new
I0813 18:39:15.769878 16416 net.cpp:126] Top shape: 256 2 1 1 (512)
I0813 18:39:15.769891 16416 net.cpp:152] fc8_new_clamp needs backward computation.
I0813 18:39:15.769897 16416 net.cpp:75] Creating Layer threshold
I0813 18:39:15.769902 16416 net.cpp:85] threshold <- fc8_new
I0813 18:39:15.769907 16416 net.cpp:85] threshold <- label
I0813 18:39:15.769913 16416 net.cpp:111] threshold -> fc8_new_thresh
I0813 18:39:15.769920 16416 net.cpp:99] threshold -> label (in-place)
I0813 18:39:15.769927 16416 net.cpp:126] Top shape: 256 2 1 1 (512)
I0813 18:39:15.769932 16416 net.cpp:126] Top shape: 256 1 1 1 (256)
I0813 18:39:15.769937 16416 net.cpp:152] threshold needs backward computation.
I0813 18:39:15.769942 16416 net.cpp:75] Creating Layer prob
I0813 18:39:15.769945 16416 net.cpp:85] prob <- fc8_new_thresh
I0813 18:39:15.769951 16416 net.cpp:111] prob -> prob
I0813 18:39:15.769966 16416 net.cpp:126] Top shape: 256 2 1 1 (512)
I0813 18:39:15.769975 16416 net.cpp:152] prob needs backward computation.
I0813 18:39:15.769985 16416 net.cpp:75] Creating Layer accuracy
I0813 18:39:15.769992 16416 net.cpp:85] accuracy <- prob
I0813 18:39:15.770005 16416 net.cpp:85] accuracy <- label
I0813 18:39:15.770045 16416 net.cpp:111] accuracy -> accuracy
I0813 18:39:15.770061 16416 net.cpp:126] Top shape: 1 2 1 1 (2)
I0813 18:39:15.770073 16416 net.cpp:152] accuracy needs backward computation.
I0813 18:39:15.770081 16416 net.cpp:163] This network produces output accuracy
I0813 18:39:15.770113 16416 net.cpp:181] Collecting Learning Rate and Weight Decay.
I0813 18:39:15.770143 16416 net.cpp:174] Network initialization done.
I0813 18:39:15.770155 16416 net.cpp:175] Memory required for Data 1073741832
I0813 18:39:15.770267 16416 solver.cpp:49] Solver scaffolding done.
I0813 18:39:15.770278 16416 finetune_net.cpp:27] Loading from ../alexnet/caffe_alexnet_model
I0813 18:39:16.566872 16416 net.cpp:319] Copying source layer data
I0813 18:39:16.566931 16416 net.cpp:319] Copying source layer conv1
I0813 18:39:16.567046 16416 net.cpp:319] Copying source layer relu1
I0813 18:39:16.567060 16416 net.cpp:319] Copying source layer norm1
I0813 18:39:16.567067 16416 net.cpp:319] Copying source layer pool1
I0813 18:39:16.567075 16416 net.cpp:319] Copying source layer conv2
I0813 18:39:16.567618 16416 net.cpp:319] Copying source layer relu2
I0813 18:39:16.567637 16416 net.cpp:319] Copying source layer norm2
I0813 18:39:16.567644 16416 net.cpp:319] Copying source layer pool2
I0813 18:39:16.567652 16416 net.cpp:319] Copying source layer conv3
I0813 18:39:16.569149 16416 net.cpp:319] Copying source layer relu3
I0813 18:39:16.569200 16416 net.cpp:319] Copying source layer conv4
I0813 18:39:16.570410 16416 net.cpp:319] Copying source layer relu4
I0813 18:39:16.570456 16416 net.cpp:319] Copying source layer conv5
I0813 18:39:16.571269 16416 net.cpp:319] Copying source layer relu5
I0813 18:39:16.571293 16416 net.cpp:319] Copying source layer pool5
I0813 18:39:16.571300 16416 net.cpp:319] Copying source layer fc6
I0813 18:39:16.696907 16416 net.cpp:319] Copying source layer relu6
I0813 18:39:16.696941 16416 net.cpp:319] Copying source layer drop6
I0813 18:39:16.696949 16416 net.cpp:319] Copying source layer fc7
I0813 18:39:16.751133 16416 net.cpp:319] Copying source layer relu7
I0813 18:39:16.751164 16416 net.cpp:319] Copying source layer drop7
I0813 18:39:16.751173 16416 net.cpp:316] Ignoring source layer fc8
I0813 18:39:16.751179 16416 net.cpp:319] Copying source layer loss
I0813 18:39:16.765497 16416 solver.cpp:61] Solving threshFineNet
I0813 18:39:16.765545 16416 solver.cpp:106] Iteration 0, Testing net
I0813 18:39:20.334844 16416 solver.cpp:142] Test score #0: 0.478894
I0813 18:39:20.334913 16416 solver.cpp:142] Test score #1: 0.925807
F0813 18:39:20.728991 16416 syncedmem.cpp:47] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
    @     0x7fe3b9b8cf9d  google::LogMessage::Fail()
    @     0x7fe3b9b8f0af  google::LogMessage::SendToLog()
    @     0x7fe3b9b8cb8c  google::LogMessage::Flush()
    @     0x7fe3b9b8f94d  google::LogMessageFatal::~LogMessageFatal()
    @           0x4ba4c7  caffe::SyncedMemory::mutable_gpu_data()
    @           0x45f9f1  caffe::Blob<>::mutable_gpu_data()
    @           0x4bd347  caffe::ConvolutionLayer<>::Forward_gpu()
    @           0x4385f0  caffe::Net<>::ForwardPrefilled()
    @           0x4484b4  caffe::Solver<>::Solve()
    @           0x412846  main
    @     0x7fe3b7724ea5  (unknown)
    @           0x414629  (unknown)
Aborted
Done.
